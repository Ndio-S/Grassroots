{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f54d73-1491-4564-bc9e-f374921a6ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fefb8-b5c7-4714-8c41-22e5ea0e6bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Wikipedia URL containing the list of football clubs in England\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_football_clubs_in_England\"\n",
    "\n",
    "# Headers to avoid being blocked\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Unable to fetch page. Status code {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all tables (since Wikipedia organizes clubs in tables)\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "club_data = []\n",
    "\n",
    "# Loop through all tables to extract club data\n",
    "for table in tables:\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\")\n",
    "        if len(cells) > 1:\n",
    "            club_name = cells[0].text.strip()\n",
    "            club_link = \"https://en.wikipedia.org\" + cells[0].find(\"a\")[\"href\"] if cells[0].find(\"a\") else \"No link available\"\n",
    "\n",
    "            # Save data to list\n",
    "            club_data.append({\n",
    "                \"Club Name\": club_name,\n",
    "                \"Wikipedia Link\": club_link\n",
    "            })\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(club_data)\n",
    "\n",
    "# Save to CSV file\n",
    "output_file = \"uk_football_clubs.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Scraped {len(club_data)} football clubs and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a69141-565b-4c98-bdf1-a1c8f0298ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced69fe-abdb-41e4-848a-e5e063c092cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0a6b2-e052-4365-85c7-fd4f7bf8f834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591db995-937b-4e5f-8323-154b204b2228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Your Google API Key (Replace with your actual key)\n",
    "GOOGLE_API_KEY = \"AIzaSyAJaAus6xYdQhWUwTqVbNO8Opxg_-7VB2M\"\n",
    "\n",
    "df = pd.read_csv('uk_football_clubs.csv')\n",
    "\n",
    "# Check if required columns exist\n",
    "if \"Club Name\" not in df.columns or \"Wikipedia Link\" not in df.columns:\n",
    "    print(\"‚ö†Ô∏è CSV is missing required columns. Ensure 'Club Name' and 'Wikipedia Link' exist.\")\n",
    "    exit()\n",
    "\n",
    "# Add missing columns if not present\n",
    "if \"Stadium Name\" not in df.columns:\n",
    "    df[\"Stadium Name\"] = None\n",
    "if \"Latitude\" not in df.columns:\n",
    "    df[\"Latitude\"] = None\n",
    "if \"Longitude\" not in df.columns:\n",
    "    df[\"Longitude\"] = None\n",
    "\n",
    "# Function to scrape stadium name from Wikipedia\n",
    "def get_stadium_name(wikipedia_url):\n",
    "    try:\n",
    "        response = requests.get(wikipedia_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        infobox = soup.find(\"table\", {\"class\": \"infobox\"})\n",
    "        if not infobox:\n",
    "            return None\n",
    "        \n",
    "        for row in infobox.find_all(\"tr\"):\n",
    "            if \"Ground\" in row.text or \"Stadium\" in row.text:\n",
    "                stadium = row.find(\"td\").text.strip()\n",
    "                return stadium\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Function to get coordinates from Google Geocoding API\n",
    "def get_coordinates(stadium, club_name):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    address = f\"{stadium}, {club_name}, UK\"\n",
    "    \n",
    "    params = {\n",
    "        \"address\": address,\n",
    "        \"key\": GOOGLE_API_KEY\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data[\"status\"] == \"OK\":\n",
    "        location = data[\"results\"][0][\"geometry\"][\"location\"]\n",
    "        return location[\"lat\"], location[\"lng\"]\n",
    "    else:\n",
    "        with open(\"failed_clubs.log\", \"a\") as log_file:\n",
    "            log_file.write(f\"{club_name} - {stadium} returned NO RESULT\\n\")\n",
    "        return None, None\n",
    "\n",
    "print(f\"Processing {len(df)} clubs...\")\n",
    "\n",
    "# Process all clubs\n",
    "for i, row in df.iterrows():\n",
    "    club_name = row[\"Club Name\"]\n",
    "    wiki_link = row[\"Wikipedia Link\"]\n",
    "\n",
    "    # Skip if we already have data\n",
    "    if pd.notna(row[\"Stadium Name\"]) and pd.notna(row[\"Latitude\"]) and pd.notna(row[\"Longitude\"]):\n",
    "        continue\n",
    "\n",
    "    # Extract stadium name from Wikipedia\n",
    "    stadium = get_stadium_name(wiki_link)\n",
    "    if not stadium:\n",
    "        with open(\"failed_clubs.log\", \"a\") as log_file:\n",
    "            log_file.write(f\"{club_name} - NO STADIUM FOUND\\n\")\n",
    "        continue\n",
    "\n",
    "    df.at[i, \"Stadium Name\"] = stadium\n",
    "\n",
    "    # Get coordinates\n",
    "    lat, lon = get_coordinates(stadium, club_name)\n",
    "    df.at[i, \"Latitude\"] = lat\n",
    "    df.at[i, \"Longitude\"] = lon\n",
    "    \n",
    "    # Print progress every 20 clubs\n",
    "    if i % 20 == 0:\n",
    "        print(f\"‚úÖ Processed {i}/{len(df)} clubs...\")\n",
    "\n",
    "    time.sleep(1.5)  # Sleep to prevent API rate limits\n",
    "\n",
    "# Save final output\n",
    "output_file = \"uk_football_clubs_with_coordinates.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nüéØ Process complete for ALL clubs! Saved as {output_file}\")\n",
    "print(\"‚ö†Ô∏è Check 'failed_clubs.log' for any missing stadiums.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
